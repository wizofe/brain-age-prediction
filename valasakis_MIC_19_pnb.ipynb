{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO407H - Medical Image Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursework - Age regression from brain MRI\n",
    "\n",
    "Predicting the age from a brain MRI scan is believed to have diagnostic value in the context of a number of pathologies that cause structural changes and damage to the brain. Assuming an accurate predictor of brain age can be trained based on a set of healthy subjects, the idea is then to compare the predicted age obtained on a new patient scan with the real age of that patient. Discrepancy between predicted and real age might indicate the presence of pathology and abnormal changes to the brain.\n",
    "\n",
    "The objective for the coursework is to implement two different supervised learning approaches for age regression from brain MRI data. Data from 652 subjects will be provided. Each approach will require a processing pipeline with different components that you will need to implement using methods that were discussed in the lectures and tutorials. There are dedicated sections in the Jupyter notebook for each approach which contain some detailed instructions and some hints and notes.\n",
    "\n",
    "For many tasks, you will find useful ideas and implementations in the tutorial notebooks. Make sure to add documentation to your code. Markers will find it easier to understand your reasoning when sufficiently detailed comments are provided in your implementations.\n",
    "\n",
    "#### Read the descriptions and provided code cells carefully and look out for the cells marked with 'TASK'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use full browser width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "The following cells provide some helper functions to load the data, and provide some overview and visualisation of the statistics over the population of 652 subjects. Let's start by loading the meta data, that is the data containing information about the subject IDs, their age, and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# WARNING: Change directories as expected!\n",
    "%cd '/home/wizofe'\n",
    "data_dir = 'mic/project-data/'\n",
    "meta_data = pd.read_csv(os.path.join(data_dir,'meta/clean_participant_data.csv'))\n",
    "meta_data.head() # show the first five data entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look the the population statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.set_cmap('gist_earth')\n",
    "plt.xkcd()\n",
    "\n",
    "sns.catplot(x=\"gender_text\", data=meta_data, kind=\"count\")\n",
    "plt.title('Gender distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(meta_data['age'], bins=[10,20,30,40,50,60,70,80,90])\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(len(meta_data['age'])),meta_data['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculate parameters low and high from window and level\n",
    "def wl_to_lh(window, level):\n",
    "    low = level - window/2\n",
    "    high = level + window/2\n",
    "    return low,high\n",
    "\n",
    "def display_image(img, x=None, y=None, z=None, window=None, level=None):\n",
    "    # Convert SimpleITK image to NumPy array\n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    \n",
    "    # Get image dimensions in millimetres\n",
    "    size = img.GetSize()\n",
    "    spacing = img.GetSpacing()\n",
    "    width  = size[0] * spacing[0]\n",
    "    height = size[1] * spacing[1]\n",
    "    depth  = size[2] * spacing[2]\n",
    "    print(width, height, depth)\n",
    "    \n",
    "    if x is None:\n",
    "        x = np.floor(size[0]/2).astype(int)\n",
    "    if y is None:\n",
    "        y = np.floor(size[1]/2).astype(int)\n",
    "    if z is None:\n",
    "        z = np.floor(size[2]/2).astype(int)\n",
    "    \n",
    "    if window is None:\n",
    "        window = np.max(img_array) - np.min(img_array)\n",
    "    \n",
    "    if level is None:\n",
    "        level = window / 2 + np.min(img_array)\n",
    "    \n",
    "    low,high = wl_to_lh(window,level)\n",
    "\n",
    "    # Display the orthogonal slices\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    ax1.imshow(img_array[z,:,:], cmap='gray', clim=(low, high), extent=(0, width, height, 0))\n",
    "    ax2.imshow(img_array[:,y,:], origin='lower', cmap='gray', clim=(low, high), extent=(0, width,  0, depth))\n",
    "    ax3.imshow(img_array[:,:,x], origin='lower', cmap='gray', clim=(low, high), extent=(0, height, 0, depth))\n",
    "\n",
    "    # Additionally display crosshairs\n",
    "    ax1.axhline(y * spacing[1], lw=1)\n",
    "    ax1.axvline(x * spacing[0], lw=1)\n",
    "    \n",
    "    ax2.axhline(z * spacing[2], lw=1)\n",
    "    ax2.axvline(x * spacing[0], lw=1)\n",
    "    \n",
    "    ax3.axhline(z * spacing[2], lw=1)\n",
    "    ax3.axvline(y * spacing[1], lw=1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def interactive_view(img):\n",
    "    size = img.GetSize() \n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    interact(display_image,img=fixed(img),\n",
    "             x=(0, size[0] - 1),\n",
    "             y=(0, size[1] - 1),\n",
    "             z=(0, size[2] - 1),\n",
    "             window=(0,np.max(img_array) - np.min(img_array)),\n",
    "             level=(np.min(img_array),np.max(img_array)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_brain(img, msk):\n",
    "    \"\"\" From a given image and brain mask, skull strip the brain image and apply normalisation\n",
    "    Args:\n",
    "        img(sitk image): The original brain image including the skull\n",
    "        msk(sitk image): A defined brain mask image\n",
    "        \n",
    "    Returns:\n",
    "        new_brain(sitk image): The resulting skull stripped brain image\n",
    "    \"\"\"\n",
    "    img_array = sitk.GetArrayFromImage(img)\n",
    "    mask_array = sitk.GetArrayFromImage(msk)\n",
    "\n",
    "    dump_skull = img_array\n",
    "    \n",
    "    # skull-strip\n",
    "    dump_skull[mask_array==0] = 0\n",
    "    # display_image(sitk.GetImageFromArray(dump_skull))\n",
    "    \n",
    "    # If a SITK Image Object is needed\n",
    "    # new_brain = sitk.GetImageFromArray(dump_skull)\n",
    "    # new_brain.CopyInformation(img)\n",
    "    \n",
    "    # normalise the image - zscore\n",
    "    dump_skull = (dump_skull - np.mean(dump_skull[mask_array>0])) / np.std(img_array[mask_array>0])\n",
    "    \n",
    "    # create an unstructured/flat array with the intensities of the brain voxels\n",
    "    X = dump_skull[mask_array>0].flatten().reshape(-1,1)\n",
    "    n_pts = len(X.flatten())\n",
    "    \n",
    "    # set the sampling rate for a random subset\n",
    "    sampling = 0.15\n",
    "    X_subset = np.random.choice(X.flatten(),int(n_pts*sampling)).reshape(-1, 1)\n",
    "    \n",
    "    # Visualise for debugging purposes\n",
    "    #display_image(skullstripped, window=400, level=200)\n",
    "\n",
    "    return dump_skull, mask_array, X, X_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "from nilabels.tools.image_colors_manipulations.relabeller import relabeller\n",
    "import sklearn.mixture as mixture\n",
    "import scipy\n",
    "\n",
    "\n",
    "def plot_gmm(x, gmm):\n",
    "    omega = gmm.weights_\n",
    "    mu = gmm.means_\n",
    "    sigma = np.sqrt(gmm.covariances_)\n",
    "    for ind in range(0,omega.shape[0]): \n",
    "        plt.plot(x,omega[ind]*scipy.stats.norm.pdf(x, mu[ind], sigma[ind]), linewidth=2, label='GMM Component '+str(ind))\n",
    "\n",
    "\n",
    "def gmm_init(ref_patient_ID):\n",
    "    \"\"\" Initialise a GMM using a base reference patient\n",
    "    \"\"\"\n",
    "    # Initialise GMM with a reference patient\n",
    "    img = sitk.ReadImage(glob.glob(os.path.join(data_dir,'images/')+'*'+ref_patient_ID+'*')[0])\n",
    "    msk = sitk.ReadImage(glob.glob(os.path.join(data_dir,'masks/')+'*'+ref_patient_ID+'*')[0])\n",
    "\n",
    "    just_brain, mask_array, X, X_subset = preprocess_brain(img,msk)\n",
    "    n_clusters = 3\n",
    "    \n",
    "    gmm = mixture.GaussianMixture(n_components=n_clusters)\n",
    "    gmm.fit(X_subset)\n",
    "    \n",
    "    return np.array(gmm.means_).reshape(-1,1)\n",
    "    \n",
    "        \n",
    "def segment_brain(skullstripped, msk_array, X, X_subset, m_init, seg_file):\n",
    "    \"\"\" Segment a skullstripped brain and save it to a file\n",
    "    Args:\n",
    "        skullstripped(numpy array): The skullstripped image in a numpy array format\n",
    "        msk_array(numpy array): The given mask image\n",
    "        X(numpy array): The flattened version of the skullstripped image array\n",
    "        X_subset(numpy array): A randomly sampled subset of the skullstripped image\n",
    "        segmentation_file(path): The path to save the segmentation\n",
    "    Returns:\n",
    "        seg_gmm (numpy array): The resulting labelled segmented image\n",
    "    \"\"\"\n",
    "\n",
    "    # WM,GM,CSF tissue classes\n",
    "    n_clusters = 3\n",
    "    \n",
    "    gmm = mixture.GaussianMixture(n_components=3, covariance_type = 'full', \n",
    "                                  max_iter = 1000, random_state = 0, means_init=m_init, warm_start = True)\n",
    "    gmm.fit(X_subset)\n",
    "    y = gmm.predict(skullstripped.flatten().reshape(-1, 1))\n",
    "    \n",
    "    # shift labels and ensure no background\n",
    "    y = y + 1 \n",
    "    y[(msk_array == 0).flatten()] = 0\n",
    "\n",
    "    lab_array = y.reshape(skullstripped.shape).astype('uint8')\n",
    "    seg_gmm = sitk.GetImageFromArray(lab_array)\n",
    "    seg_gmm.CopyInformation(img)\n",
    "    \n",
    "    #display_image(sitk.LabelToRGB(seg_gmm))\n",
    "    sitk.WriteImage(seg_gmm, seg_file)\n",
    "    #print('Wrote file!')\n",
    "\n",
    "    lim_low = np.min(X).astype(np.int)\n",
    "    lim_high = np.max(X).astype(np.int)\n",
    "    num_bins = 400\n",
    "\n",
    "    return seg_gmm\n",
    "\n",
    "# Histogram GMM plot\n",
    "#     plt.figure(figsize=(10, 4), dpi=100)\n",
    "#     plt.hist(X, bins=num_bins, density=True, range=(lim_low, lim_high), label='Intensity histogram', color='tomato');\n",
    "#     x = np.linspace(lim_low,lim_high,num_bins).reshape(-1,1)\n",
    "#     plot_gmm(x,gmm)\n",
    "#     plt.plot(x,np.exp(gmm.score_samples(x)), linewidth=1.5, color='k', label='Gaussian Mixture Model')\n",
    "#     plt.xlim([lim_low,lim_high])\n",
    "#     plt.legend(loc=0, shadow=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Shape statistics filter (for volume information)\n",
    "global shape_stats \n",
    "shape_stats = sitk.LabelShapeStatisticsImageFilter()\n",
    "\n",
    "# create a pandas dataframe\n",
    "global df_main\n",
    "df_main = pd.DataFrame()\n",
    "\n",
    "def store_features(seg):\n",
    "    \"\"\" Store image brain shape statistics in a csv file (using pandas DF)\n",
    "    Args:\n",
    "        seg(sitk image): The segmentation image in SITK Image object format\n",
    "        df_main(pandas dataframe): The main pandas dataframe for feature saving\n",
    "        shape_stats(sitk object): The feature statistics object from SITK\n",
    "    Returns:\n",
    "        df_main(pandas dataframe): The final dataframe with all the features\n",
    "    \"\"\"\n",
    "    # Other features such as intensity statistics can also be extracted\n",
    "    # intensity_stats = sitk.LabelIntensityStatisticsImageFilter()\n",
    "    shape_stats.Execute(seg)\n",
    "    \n",
    "    stats_list = [(shape_stats.GetPhysicalSize(i)) \n",
    "                  for i in shape_stats.GetLabels()]\n",
    "    total_volume = np.sum(stats_list)  # Compute total volume to obtain the volume ratio for each structure\n",
    "    \n",
    "    cols = ['Volume ratio']\n",
    "    data = {'Total Volume':total_volume}\n",
    "    vol_df = pd.DataFrame(data, index=[0])  \n",
    "    stats_list = stats_list/total_volume\n",
    "    \n",
    "    stats = pd.DataFrame(data=stats_list, index=shape_stats.GetLabels(), \n",
    "                         columns=cols)\n",
    "    stats_v = pd.concat([stats.iloc[0], stats.iloc[1], \n",
    "                         stats.iloc[2], vol_df.iloc[0]], axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame(stats_v.values.reshape(1, len(stats_list)+1), \n",
    "                      columns = stats_v.index)\n",
    "    df_main.append(df)\n",
    "    \n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK A-2: Feature calculation\n",
    "\n",
    "Implement a function that calculates volume features given the three tissue volumes and the overal brain volume (which can be calculated from the brain masks). You should use this function to construct a big matrix $X$ with a row for each subject and features across the columns.\n",
    "\n",
    "*Note:* You may need to experiment here what the best set of features is. Start with just calculating three simple features of relative tissue volumes. So you should initially construct an $X^{652 \\times 3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Initialise the GMM with a reference patient\n",
    "\n",
    "ref_patient_ID = 'CC110037'\n",
    "m_init = gmm_init(ref_patient_ID)\n",
    "\n",
    "# Remove all the comments if display is desirable\n",
    "# Disabled by default as for data limits on Jupyter Notebooks\n",
    "\n",
    "for i in range(meta_data['ID'].count()):\n",
    "    # Subject with index 0\n",
    "    ID = meta_data['ID'][i]\n",
    "    age = meta_data['age'][i]\n",
    "\n",
    "    # Data folders\n",
    "    image_dir = os.path.join(data_dir, 'images/')\n",
    "    image_filenames = glob.glob(image_dir + '*.nii.gz')\n",
    "\n",
    "    mask_dir = os.path.join(data_dir, 'masks/')\n",
    "    mask_filenames = glob.glob(mask_dir + '*.nii.gz')\n",
    "\n",
    "    greymatter_dir = os.path.join(data_dir, 'greymatter/')\n",
    "    greymatter_filenames = glob.glob(greymatter_dir + '*.nii.gz')\n",
    "\n",
    "    image_filename = [f for f in image_filenames if ID in f][0]\n",
    "    img = sitk.ReadImage(image_filename)\n",
    "\n",
    "    mask_filename = [f for f in mask_filenames if ID in f][0]\n",
    "    msk = sitk.ReadImage(mask_filename)\n",
    "\n",
    "    greymatter_filename = [f for f in greymatter_filenames if ID in f][0]\n",
    "    gm = sitk.ReadImage(greymatter_filename)\n",
    "\n",
    "    print('Imaging data of subject ' + ID + ' with age ' + str(age))\n",
    "\n",
    "    print('\\nMR Image (used in part A)')\n",
    "    #display_image(img, window=400, level=200)\n",
    "    \n",
    "    print('Brain mask (used in part A)')\n",
    "    #display_image(msk)\n",
    "\n",
    "    #print(image_filename)\n",
    "    print('Spatially normalised grey matter maps (used in part B)')\n",
    "    #display_image(gm)\n",
    "\n",
    "    # Ensure the registrations directory is created\n",
    "    # if not exists\n",
    "    rd = os.path.join(data_dir, 'segm/')\n",
    "    if not os.path.exists(rd):\n",
    "        os.makedirs(rd)\n",
    "\n",
    "    # Storage location for the segmented file\n",
    "    seg_file = os.path.join(rd, os.path.basename(ID + '.nii.gz'))\n",
    " \n",
    "    # Preprocess the brain image\n",
    "    just_brain, mask_array, X, X_subset = preprocess_brain(img,msk)\n",
    "    seg_im = segment_brain(just_brain, mask_array, X, X_subset, m_init, seg_file)\n",
    "    \n",
    "    # Store the volumes in a CSV file\n",
    "    sf = store_features(seg_im)\n",
    "    \n",
    "    display_image(sitk.GetImageFromArray(just_brain))\n",
    "    display_image(sitk.LabelToRGB(seg_im))\n",
    "\n",
    "#     # Plot the histogram before and after normalisation\n",
    "#     plt.hist(skullstripped, bins=200, density=True)\n",
    "#     plt.figure()\n",
    "#     plt.hist((skullstripped-np.mean(skullstripped)) / np.std(skullstripped), bins=200, density=True)\n",
    "\n",
    "# Store the features to a csv file \n",
    "sf.to_csv(os.path.join(data_dir, 'features.csv'), index = 0)\n",
    "\n",
    "# A reminder, useful for Jupyter notebooks.\n",
    "print('Finished Processing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
    "    \"\"\"Resamples an image to given element spacing and output size.\n",
    "    Modified from the KiTS19-Challenge, original code by Junqiang Chen (public domain)\n",
    "    \"\"\"\n",
    "\n",
    "    original_spacing = np.array(image.GetSpacing())\n",
    "    original_size = np.array(image.GetSize())\n",
    "\n",
    "    if out_size is None:\n",
    "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
    "    else:\n",
    "        out_size = np.array(out_size)\n",
    "\n",
    "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
    "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
    "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
    "\n",
    "    original_center = np.matmul(original_direction, original_center)\n",
    "    out_center = np.matmul(original_direction, out_center)\n",
    "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size.tolist())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(out_origin.tolist())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(pad_value)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# set the voxels to isotropic resolution of 8mm\n",
    "img_size = [64, 64, 64]\n",
    "img_res = [8, 8, 8]\n",
    "\n",
    "age = list(meta_data['age'])\n",
    "\n",
    "# The total volumns, normalised and age-related\n",
    "# ageVolumn = {age: [csf,gm,wm]} (dictionary of ages to CSF/GM/WM)\n",
    "ageVolumn = defaultdict(np.array)\n",
    "volTotal = img_res[0]**2 + img_res[1]**2+ img_res[2]**2\n",
    "vols_normalised = np.zeros((3, meta_data['ID'].count()))\n",
    "\n",
    "for i in range(meta_data['ID'].count()):\n",
    "    ID = meta_data['ID'][i]\n",
    "    img = sitk.ReadImage(os.path.join(rd, os.listdir(rd)[i]))\n",
    "    seg = resample_image(img, img_res, img_size)\n",
    "\n",
    "    img_array = sitk.GetArrayFromImage(img) \n",
    "    seg_array = sitk.GetArrayFromImage(seg)\n",
    "\n",
    "    frq = Counter(seg_array.flatten()) \n",
    "    unit = img_res[0] * img_res[1] * img_res[2]\n",
    "    vols_normalised[:,i] = np.array([frq[1]*unit/volTotal, frq[2]*unit/volTotal,frq[3]*unit/volTotal])\n",
    "    \n",
    "    ageVolumn[meta_data['age'][i]] = ageVolumn.get(meta_data['age'][i], np.array([0,0,0])) + np.array([frq[1]*unit, frq[2]*unit, frq[3]*unit])\n",
    "\n",
    "# Plot the age against the normalised volume data\n",
    "# plt.xkcd()\n",
    "plt.figure()\n",
    "plt.title('Features vs age', fontsize=14)\n",
    "plt.xlabel('Age', fontsize = 14)\n",
    "plt.ylabel('Relative normalised volume', fontsize = 14)\n",
    "plt.legend(['CSF', 'GM', 'WM'], fontsize = 12)\n",
    "\n",
    "plt.scatter(age,vols_normalised[0,:])\n",
    "plt.scatter(age,vols_normalised[1,:])\n",
    "\n",
    "plt.scatter(age,vols_normalised[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "\n",
    "# SKLearn returns a lot of deprecated warnings, disable\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "def scatter_plot(title, y_test, y_pred):\n",
    "    \"\"\" Plot true labels vs. predicted labels \n",
    "    Args:\n",
    "        title (str): The title of the plot\n",
    "        y_test (numpy array): The test values of the data set\n",
    "        y_pred (numpy array): The predictions fitted by the model\n",
    "    \n",
    "    Return:\n",
    "        It displays the requested plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.xkcd() # Some fun\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title, fontsize=13)\n",
    "    \n",
    "    # Thanks to https://stackoverflow.com/a/34004236\n",
    "    plt.rcParams['xtick.labelsize'] = 9\n",
    "    plt.rcParams['ytick.labelsize'] = 9\n",
    "\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.scatter(y_test, y_test, c='xkcd:bubblegum')\n",
    "    \n",
    "    plt.xlabel('True Age', fontsize = 11)\n",
    "    plt.ylabel('Predicted Age', fontsize = 12)\n",
    "    \n",
    "    plt.legend(['Model', 'True'], fontsize = 9, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_linear_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Train a Linear Regression Classifier\n",
    "    Args:\n",
    "        X_train (numpy array): input volume data\n",
    "        y_train (numpy array): input age training data\n",
    "        X_test (numpy array): volume test data\n",
    "        y_test (numpy array): age test data\n",
    "    Returns:\n",
    "        model (object): A Linear Regression Model object\n",
    "        R2L (float): The model R2 score\n",
    "        RMSEL (float): The model root mean square error\n",
    "    \"\"\"\n",
    "    R2L = []\n",
    "    RMSEL = []\n",
    "    \n",
    "    LinearModel = linear_model.LinearRegression()\n",
    "    LinearModel.fit(X_train, y_train.ravel())\n",
    "  \n",
    "    R2 = LinearModel.score(X_test, y_test)\n",
    "    R2L.append(R2)   \n",
    "    \n",
    "    y_pred = LinearModel.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    RMSEL.append(rmse)\n",
    "    \n",
    "    scatter_plot('Linear Regression', y_test, y_pred)\n",
    "    \n",
    "    return LinearModel, np.around(np.mean(R2L),decimals=4)*100, np.mean(RMSEL)\n",
    "\n",
    "\n",
    "def train_bayessian_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Train a Bayessian Regression Classifier\n",
    "    Args:\n",
    "        X_train (numpy array): input volume data\n",
    "        y_train (numpy array): input age training data\n",
    "        X_test (numpy array): volume test data\n",
    "        y_test (numpy array): age test data\n",
    "    Returns:\n",
    "        model (object): A Linear Regression Model object\n",
    "        R2B (float): The model R2 score\n",
    "        RMSEB (float): The model root mean square error\n",
    "    \"\"\"\n",
    "    RMSEB = []\n",
    "    R2B = []\n",
    "    \n",
    "    BayesianModel = linear_model.BayesianRidge()\n",
    "    BayesianModel.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    #score_Br.append(BayesianModel.score(X_test,y_test))\n",
    "    y_true = y_test\n",
    "    y_pred = BayesianModel.predict(X_test)\n",
    "    \n",
    "    #mean_error_Br.append(mean_absolute_error(y_true, y_pred))\n",
    "    RMSEB.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    R2B.append(r2_score(y_true, y_pred))\n",
    "\n",
    "    scatter_plot('Bayesian Regression', y_test, y_pred)\n",
    "    \n",
    "    return BayesianModel, np.around(np.mean(R2B),decimals=4)*100, np.mean(RMSEB)\n",
    "    \n",
    "# Fit and evaluate SVR with RBF kernel model. \n",
    "# Use grid search to find the best combination of hyperparameters. \n",
    "def train_SVR_RBF(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Train a Bayessian Regression Classifier\n",
    "    Args:\n",
    "        X_train (numpy array): input volume data\n",
    "        y_train (numpy array): input age training data\n",
    "        X_test (numpy array): volume test data\n",
    "        y_test (numpy array): age test data\n",
    "    Returns:\n",
    "        model (object): A Linear Regression Model object\n",
    "        R2_SVR (float): The model R2 score\n",
    "        RMSE_SVR(float): The model root mean square error\n",
    "    \"\"\"\n",
    "    R2_SVR = []\n",
    "    RMSE_SVR = []\n",
    "    \n",
    "    svr = svm.SVR(kernel = 'rbf')\n",
    "    param_grid = {\"epsilon\": np.logspace(-10, 5, 16), \"C\": [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  \"gamma\": np.logspace(-10, 5, 16)}\n",
    "    SVRModel = GridSearchCV(svr, cv=2, scoring='neg_mean_squared_error', param_grid=param_grid)\n",
    "\n",
    "    # grid search to find the best h-parameteres\n",
    "    SVRModel.fit(X_train,y_train.ravel())\n",
    "    SVRModel = SVRModel.best_estimator_\n",
    "    y_pred = SVRModel.predict(X_test)\n",
    "    \n",
    "    R2_SVR.append(SVRModel.score(X_test, y_test))\n",
    "    RMSE_SVR.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    scatter_plot('SVR with radial basis function kernel', y_test, y_pred)\n",
    "\n",
    "    return SVRModel, np.around(np.mean(R2_SVR),decimals=4)*100, np.mean(RMSE_SVR)\n",
    "\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Train a Gradient Boosting Model\n",
    "    Args:\n",
    "        X_train (numpy array): input volume data\n",
    "        y_train (numpy array): input age training data\n",
    "        X_test (numpy array): volume test data\n",
    "        y_test (numpy array): age test data\n",
    "    Returns:\n",
    "        model (object): A Linear Regression Model object\n",
    "        R2_GB (float): The model R2 score\n",
    "        RMSE_GB (float): The model root mean square error\n",
    "    \"\"\"\n",
    "    R2_GB = []\n",
    "    RMSE_GB = []\n",
    "    \n",
    "    regressor = ensemble.GradientBoostingRegressor(random_state=0, loss='ls')\n",
    "    parameters = {'n_estimators':(100,300,500),'learning_rate':(0.01, 0.1,0.2,0.3,0.4,0.5),'max_depth':(1,3,5,7)}\n",
    "    scoring_function = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    reg = GridSearchCV(regressor, parameters, scoring_function)\n",
    "    \n",
    "    reg.fit(X_train, y_train.ravel())\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    #R2_GB.append(reg.score(X_test, y_test))\n",
    "    R2_GB.append(r2_score(y_train, y_pred))\n",
    "    RMSE_GB.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    scatter_plot('Gradient boosting', y_test, y_pred)\n",
    "\n",
    "    return reg.best_estimator_, np.around(np.mean(R2_GB),decimals=4)*100, np.mean(RMSE_GB)\n",
    "\n",
    "\n",
    "def train_decision_tree(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Train a Decision Tree Model\n",
    "    Args:\n",
    "        X_train (numpy array): input volume data\n",
    "        y_train (numpy array): input age training data\n",
    "        X_test (numpy array): volume test data\n",
    "        y_test (numpy array): age test data\n",
    "    Returns:\n",
    "        model (object): A Linear Regression Model object\n",
    "        R2_DT (float): The model R2 score\n",
    "        RMSE_DT (float): The model root mean square error\n",
    "    \"\"\"\n",
    "    R2_DT = []\n",
    "    R2_DT_ada = []\n",
    "    RMSE_DT = []\n",
    "    RMSE_DT_ada = []\n",
    "    \n",
    "    regressor = tree.DecisionTreeRegressor(max_depth=8)\n",
    "    regressor_ada = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=8),\n",
    "                          n_estimators=300)\n",
    "\n",
    "    regressor.fit(X_train, y_train.ravel())\n",
    "    regressor_ada.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    y_pred_ada = regressor_ada.predict(X_test)\n",
    "\n",
    "    R2_DT.append(r2_score(y_train, y_pred))\n",
    "    R2_DT_ada.append(r2_score(y_train, y_pred_ada))\n",
    "    \n",
    "    RMSE_DT.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    RMSE_DT_ada.append(np.sqrt(mean_squared_error(y_test, y_pred_ada)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Decision Tree using Ada Boost', fontsize=13)\n",
    "    \n",
    "    # Thanks to https://stackoverflow.com/a/34004236\n",
    "    plt.rcParams['xtick.labelsize'] = 9\n",
    "    plt.rcParams['ytick.labelsize'] = 9\n",
    "\n",
    "    \n",
    "    plt.scatter(y_train, y_pred)\n",
    "    plt.scatter(y_train, y_pred_ada, c='xkcd:seafoam')\n",
    "    \n",
    "    plt.scatter(y_test, y_test, c='xkcd:bubblegum')\n",
    "    \n",
    "    plt.xlabel('True Age', fontsize = 11)\n",
    "    plt.ylabel('Predicted Age', fontsize = 12)\n",
    "    \n",
    "    plt.legend(['Model', 'Model ADA', 'True'], fontsize = 9, loc='upper left')\n",
    "    plt.show()\n",
    "    print(np.mean(R2_DT_ada)*100)\n",
    "    return regressor, np.around(np.mean(R2_DT),decimals=4)*100, np.mean(RMSE_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(os.path.join(data_dir, 'features.csv'))\n",
    "X = np.array(X_df)\n",
    "\n",
    "# Read labels from meta-data\n",
    "y = [meta_data['age'][i] for i in range(len(meta_data))]\n",
    "y = np.reshape(y,(len(meta_data),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Regression time! :) \n",
    "# Regressions (Linear, Bayessian Ridge, SVR with RBF)\n",
    "# Ensure that package is installed\n",
    "#!pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Subset of X with the total volume\n",
    "X_sub = X[:, 0:3]\n",
    "\n",
    "# Test X just for GM and WM\n",
    "# Note: You can comment out the X_sub above and replace with this \n",
    "# Best results though were achieved using the combination of all volumes! \n",
    "# X_sub = np.transpose(np.vstack([X[:,1].ravel(), X[:,2].ravel()]))\n",
    "\n",
    "# Even though it can use the train functions inside one loop\n",
    "# It's much faster to perform k-fold multiple times\n",
    "# and create multiple loops\n",
    "\n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for train_index, test_index in kf.split(X_sub):\n",
    "    X_train, X_test = X_sub[train_index], X_sub[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "    _, R2L, RMSEL = train_linear_regression(X_train, y_train, X_test, y_test)\n",
    "\n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for train_index, test_index in kf.split(X_sub):\n",
    "    X_train, X_test = X_sub[train_index], X_sub[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "    _, R2B, RMSEB = train_bayessian_regression(X_train, y_train, X_test, y_test)\n",
    "\n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for train_index, test_index in kf.split(X_sub):\n",
    "    X_train, X_test = X_sub[train_index], X_sub[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "    _, R2_SVR, RMSE_SVR = train_SVR_RBF(X_train, y_train, X_test, y_test)\n",
    "\n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for train_index, test_index in kf.split(X_sub):\n",
    "    X_train, X_test = X_sub[train_index], X_sub[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "    _, R2_GB, RMSE_GB = train_gradient_boosting(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for train_index, test_index in kf.split(X_sub):\n",
    "    X_train, X_test = X_sub[train_index], X_sub[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]   \n",
    "\n",
    "    _, R2_DT, RMSE_DT = train_decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the data values in tabular format\n",
    "metric_values = [[\"Linear Regression\", R2L, RMSEL], [\"Bayessian Regression\", R2B, RMSEB], [\"SVR with RBF\", R2_SVR, RMSE_SVR], [\"Gradient Boosting\", R2_GB, RMSE_GB], [\"Decision Tree\", R2_DT, RMSE_DT]]\n",
    "titles = [\"Regression\", \"r2-score (mean, %)\", \"RMSE (mean)\"]\n",
    "\n",
    "print(tabulate(metric_values, headers=titles, colalign=(\"center\", \"center\", \"center\"), tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Image-based regression using grey matter maps\n",
    "The second approach will make use of grey matter maps that have been already extracted from the set of MRI scans and aligned to a common reference space to obtain spatially normalised maps. For this, we have used an advanced, state-of-the-art neuroimaging toolkit, called SPM12. The reference space corresponds to the MNI atlas (compare slide 99 of the Segmentation lecture).\n",
    "\n",
    "Because the grey matter maps are spatially normalised, voxel locations across images from different subjects roughly correspond to the same anatomical locations. This means that each voxel location in the grey matter maps can be treated as an individual feature. Because those maps are quite large, there would be a very large number of features to deal with. A dimensionality reduction using PCA needs to be performed before training a suitable regressor on the low-dimensional feature representation obtained with PCA. It might also be beneficial to apply some pre-processing before running PCA, which should be explored. The implemented pipeline should be evaluated using cross-validation using the same data splits as in part A, so the two approaches can be directly compared.\n",
    "\n",
    "Note: For part B, only the spatially normalised grey matter maps should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-1: Pre-processing\n",
    "\n",
    "Before running PCA to reduce the dimensionality of the feature space for grey matter maps, it might be beneficial to run some pre-processing on the maps. In voxel-based analysis, such as training a regressor where each voxel location is a feature, it is common to apply some smoothing beforehand. This is to reduce noise and to compensate for errors of the spatial normalisation that had been applied to the maps.\n",
    "\n",
    "Because the maps are quite large, it might also be worthwile to explore whether downsampling could be performed, before PCA. This would further reduce the dimensionality, and might be even needed in the case where PCA on the orignial resolution runs into memory issues.\n",
    "\n",
    "Implement a function that performs suitable pre-processing on each grey matter map.\n",
    "\n",
    "*Hint:* Check out tutorial 1. You may want to save the pre-processed maps using `sitk.WriteImage` to avoid recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from skimage import transform\n",
    "import glob\n",
    "\n",
    "def preprocess_gray_matter(gm, us_factor):\n",
    "    \"\"\" Preprocess each gray matter image by \n",
    "    Args:\n",
    "        gm(sitk image): The graymatter image file\n",
    "        us_factor(float): An undersampling factor\n",
    "    Returns:\n",
    "        The downsampled image using local averaging\n",
    "    \"\"\"\n",
    "    # for a sigma value chosen experimentally\n",
    "    for i in range(gm.shape[2]):\n",
    "        gm[:,:,i] = ndimage.gaussian_filter(gm[:,:,i], sigma=0.65)\n",
    "\n",
    "    return transform.downscale_local_mean(gm, factors = (us_factor,us_factor,us_factor))\n",
    "\n",
    "def store_preprocessed(us_factor):\n",
    "    if not os.path.exists(os.path.join(data_dir, 'gm{}/'.format(us_factor))):\n",
    "        os.mkdir(os.path.join(data_dir, 'gm{}/'.format(us_factor)))\n",
    "        \n",
    "        return os.path.join(data_dir, 'gm{}/'.format(us_factor))\n",
    "    else:    \n",
    "        return os.path.join(data_dir, 'gm{}/'.format(us_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_factors = np.arange(1,8)\n",
    "\n",
    "# plt.rcdefaults()\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_style('white')\n",
    "# plt.set_cmap('gist_earth')\n",
    "\n",
    "for each_factor in test_factors: \n",
    "    pre_store = store_preprocessed(each_factor)\n",
    "    for i in range(meta_data['ID'].count()):\n",
    "        ID = meta_data['ID'][i]\n",
    "        age = meta_data['age'][i]\n",
    "\n",
    "        gmd = os.path.join(data_dir, 'greymatter/')\n",
    "        gm_filenames = glob.glob(gmd + '*.nii.gz')\n",
    "\n",
    "        gm_filename = [f for f in gm_filenames if ID in f][0]\n",
    "        gm = sitk.ReadImage(gm_filename)\n",
    "        #print(gm_filename)\n",
    "        gm_array = sitk.GetArrayFromImage(gm)\n",
    "\n",
    "        # process each file with each factor\n",
    "        gm_smooth_array =  preprocess_gray_matter(gm_array, each_factor)\n",
    "        gm_smooth = sitk.GetImageFromArray(gm_smooth_array)\n",
    "\n",
    "        #gm_smooth.CopyInformation(gm) \n",
    "        #print(pre_store)\n",
    "        #print(os.path.join(pre_store, os.path.basename(ID + '.nii.gz')))\n",
    "        gm_name = os.path.join(pre_store, os.path.basename(ID + '.nii.gz'))\n",
    "\n",
    "        #display_image(gm)\n",
    "\n",
    "        #display_image(gm_smooth)\n",
    "        sitk.WriteImage(gm_smooth, gm_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_factors = np.arange(1,3)\n",
    "\n",
    "# Create a dictionary storing all the features\n",
    "# Using custom specified test factors\n",
    "\n",
    "keys = ['X_PCA_'+ str(i) for i in test_factors]\n",
    "values = [[] for i in test_factors]\n",
    "feat_dict = dict(zip(keys, values))\n",
    "\n",
    "i=1\n",
    "for key, values in feat_dict.items():\n",
    "    for j in range(meta_data['ID'].count()):\n",
    "        ID = meta_data['ID'][j]\n",
    "        age = meta_data['age'][j]\n",
    "\n",
    "        gm_dir = os.path.join(data_dir, 'gm{}/'.format(i))\n",
    "        gm_filenames = glob.glob(gm_dir + '*.nii.gz')\n",
    "\n",
    "        gm_filename = [f for f in gm_filenames if ID in f][0]\n",
    "        \n",
    "        gm = sitk.ReadImage(gm_filename)\n",
    "        gm_array = sitk.GetArrayFromImage(gm)\n",
    "        \n",
    "        feat_dict[key].append(np.ndarray.flatten(gm_array))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(feat_dict[key])\n",
    "    X_PCA = scaler.transform(feat_dict[key])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-2: Dimensionality reduction\n",
    "\n",
    "Implement dimensionality reduction for grey matter maps using [scitkit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html#pca). PCA has an option to set the percentage of variance that needs to be preserved (by setting the parameter `n_components` to a value between 0 and 1). The number principal modes, that is the new dimensionality of the data, is then automatically determined. Try initially to preserve 95% of the variance (`n_components=0.95`).\n",
    "\n",
    "*Note:* When dimensionality reduction is used as pre-processing step for supervised learning, as in this case, it is important that PCA is fitted to the training data only, but then applied to both the training and testing data. So make sure your implementation consists of two separate steps, 1) fitting the PCA model to $X_{\\text{train}}$ (using the `fit` function), and 2) applying dimensionality reduction to $X_{\\text{train}}$ and $X_{\\text{test}}$ using the `transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomp\n",
    "\n",
    "def PCA(X_train, X_test):\n",
    "    \"\"\" Perform a PCA by preserving the variance percentage to 95%\n",
    "    Args:\n",
    "        X_train (np array): The training data\n",
    "        X_test (np.array): The testing input data\n",
    "    Returns:\n",
    "        X_train_pca (np.array): The PCA result on train data\n",
    "        X_test_pca (np.array): The PCA result on test data\n",
    "    \"\"\"\n",
    "    pca = decomp.PCA(n_components = 0.95)\n",
    "\n",
    "    # Fit the data\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    cum_sum=np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.plot(cum_sum)\n",
    "    plt.xticks(np.arange(0, pca.explained_variance_ratio_.shape[0],5))\n",
    "    plt.title('PCA eigenvalues cummulative distribution')\n",
    "    plt.ylabel('Cummulative Sum')\n",
    "    plt.ylabel('Cummulative Sum')\n",
    "    \n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-3: Age regression and cross-validation\n",
    "\n",
    "Experiment with different regression methods from the [scikit-learn toolkit](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Evaluate the methods using two-fold [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) in the same way as for approach A so results can be directly compared.\n",
    "\n",
    "Try using at least two different regression methods.\n",
    "\n",
    "*Note:* Remember, when you use cross-validation where you swap training and testing sets in each fold, you need to fit PCA to the training set of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X_sub = X[:,0].reshape(-1,1)\n",
    "X_sub = np.array(feat_dict['X_PCA_2'])\n",
    "\n",
    "# Remove comments to run all the regressions\n",
    "\n",
    "# # Linear Regression\n",
    "# kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "# kf.get_n_splits(X_sub)\n",
    "\n",
    "# for key, values in feat_dict.items():     \n",
    "#     X_PCA = np.array(feat_dict[key])\n",
    "#     for train_index, test_index in kf.split(X_sub):\n",
    "#         X_train, X_test = X_PCA[train_index], X_PCA[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "#         X_train_pca, X_test_pca = PCA(X_train, X_test)\n",
    "\n",
    "#         _, R2L, RMSEL = train_linear_regression(X_train_pca, y_train, X_test_pca, y_test)\n",
    "\n",
    "# Decision Tree with ADA Boost\n",
    "# kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "# kf.get_n_splits(X_sub)\n",
    "\n",
    "# for key, values in feat_dict.items():     \n",
    "#     X_PCA = np.array(feat_dict[key])\n",
    "#     for train_index, test_index in kf.split(X_sub):\n",
    "#         X_train, X_test = X_PCA[train_index], X_PCA[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "#         X_train_pca, X_test_pca = PCA(X_train, X_test)\n",
    "\n",
    "#         _, R2_DT, RMSE_DT = train_decision_tree(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# SVR RBF\n",
    "kf = KFold(n_splits=2, random_state = False, shuffle = True)\n",
    "kf.get_n_splits(X_sub)\n",
    "\n",
    "for key, values in feat_dict.items():     \n",
    "    X_PCA = np.array(feat_dict[key])\n",
    "    for train_index, test_index in kf.split(X_sub):\n",
    "        X_train, X_test = X_PCA[train_index], X_PCA[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]   \n",
    "        \n",
    "        X_train_pca, X_test_pca = PCA(X_train, X_test)\n",
    "        \n",
    "        _, R2_SVR, RMSE_SVR = train_SVR_RBF(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the data values in tabular format\n",
    "metric_values = [[\"Linear Regression\", R2L, RMSEL], [\"SVR with RBF\", R2_SVR, RMSE_SVR], [\"Decision Tree\", R2_DT, RMSE_DT]]\n",
    "titles = [\"Regression\", \"r2-score (mean, %)\", \"RMSE (mean)\"]\n",
    "\n",
    "print(tabulate(metric_values, headers=titles, colalign=(\"center\", \"center\", \"center\"), tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
